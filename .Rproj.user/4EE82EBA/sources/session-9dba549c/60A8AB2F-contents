---
title: "Tutorial"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(ybLN)
library(bench)
```

# Overview

This package comprises two primary functions: `yblm` and `ybanova`. The `yblm` function is meticulously engineered to conduct simple linear regression analysis, a pivotal statistical approach for deciphering the dynamics between an independent variable (explanatory) and a dependent variable (response). Simple linear regression fundamentally aims to identify the most fitting linear connection among observed data points. This is typically represented by the equation $\hat{y} = b_0 + b_1X$, where $\hat{y}$ symbolizes the predicted response.

Executing simple linear regression analysis in R entails several critical steps. Initially, data is imported into R, and it's paramount to verify that this data adheres to the prerequisites for linear regression, including linear association, independence, homoscedasticity (uniform error variance), and error normality. Following the preliminary data setup, linear regression analysis is usually executed using R's `lm()` function, which ascertains the correlation between variables.

R's `lm()` function is a conventional utility for linear regression, applied to model the relationship between a dependent variable and one or more independent variables. Specifically for simple linear regression, with a sole independent variable, `lm()` determines the optimal line or curve that minimizes the vertical discrepancies between the actual data points and the projected regression line, effectively minimizing the residuals or the differences between observed and model-predicted values.

The `yblm` function is another version of this methodology, offering enhanced flexibility and a more extensive array of outputs compared to the regular `lm()` function. It includes a feature to optionally integrate an intercept into the model (controlled by the `inter` parameter) and conducts checks to confirm the compatibility in dimensions of the independent variable `X` and the dependent variable `Y`. Post model fitting, `yblm` computes an array of statistical measures, including the model residuals, mean squared error (MSE), model and residual degrees of freedom, standard errors of coefficients, t-values, p-values, as well as R-squared and adjusted R-squared values. Additionally, it calculates the F-statistic and its corresponding p-value. Such an extensive output spectrum serves to facilitate a thorough analysis and diagnostic evaluation of the linear model.

The `ybanova` function is another version of `anova()` function in R. This function will generate an ANOVA table, which is similar to the R's regular function.

Further more, this package includes a sample dataset called `sampledata`, which includes `X = (X1,X2,X3), Y` where X_i,Y follows standard normal distribution and they are independent with each other. In this dataset, we have 100 X and Ys as a sample data for the function. Also, in the `testthat`, we used `iris` as a real world sample dataset to help justify the correctness of the functions in this package.

# Install Package
```{r}
devtools::install_github("Megumi-ybb/BIOSTAT625HW3")
```

# Introduction of the `yblm` in the package

First, we need to know that this function is generally used in form of `yblm(X,Y,inter)`. `X` is a numeric matrix or data frame representing the independent variables.`Y` is a numeric vector or data frame representing the dependent variable. And `inter` is logical if TRUE, includes an intercept in the model, otherwise fits without an intercept.

To know more about this package we can have two examples, we will go through the detail of them and justify the correctness and efficiency at the same time.

In the first example, we will use the `sampledata` that is already implemented in the R package, called `sampledata`.

```{r,message = FALSE}
library(ybLN)
library(bench)
example_data1 = sampledata
#You can run the code to check the dataset, we comment out the code since it is too long to be viewed in Rmd
#example_data1
```

We can see the sample data above. Then, we will set X = (X1,X2,X3), Y = Y from the example data. And then, we will run the linear regression on the example dataset using both `yblm()` and `lm()`. We will first try the regression with intercept.

```{r, collapse=TRUE,message = FALSE}
X = as.matrix(example_data1[,1:3])
Y = as.matrix(example_data1[,4])
#fit1 is the result generated by lm()
fit1 = lm(Y~X,data = example_data1)

#fityb is the result generated by yblm()
fityb1 = yblm(X,Y,TRUE)
# A matrix containing value, Estimate standard error, t value and Pr(>|t|) of each coefficients
fityb1$coef_matrix
# The predicted values by the model
#You can run the code to check the dataset, we comment out the code since it is too long to be viewed in Rmd
#fityb1$predict
# The residuals, which is response minus predicted values.
#You can run the code to check the dataset, we comment out the code since it is too long to be viewed in Rmd
#fityb1$residual
# Sum of residuals
fityb1$SS_X
# The residual degree of freedom.
fityb1$R_df
# The degree of freedom between groups.
fityb1$M_df
# The standard error of the residual
fityb1$RSE
# The R square score.
fityb1$R_square
# The adjusted R square
fityb1$Adj_R_square
# The F statistic
fityb1$F_statistic
# The coefficient of the variables in the model
fityb1$coef
# p value of F-statistic
fityb1$p_value_F
```

We can see that `yblm` will return a list of objects that is informative in the linear regression, including predicted value, p-value and so on. Then we will compare the solution from the `yblm` and `lm` to justify the correctness and efficiency. As we are comparing floating number., we will set tolerance to compare two floating numbers.

```{r}
library(testthat)
#----------------------------------------Test result when we have intercept--------------------
test_that("yblm returns correct coefficient", {
  tolerance = 1e-12
  expect_equal(as.numeric(fityb1$coef), unname(fit1$coefficients),tolerance = tolerance)
})

test_that("yblm returns correct predicted value", {
  tolerance = 1e-12
  expect_equal(as.numeric(fityb1$predict), unname(fit1$fitted.values),tolerance = tolerance)
})

test_that("yblm returns correct residual value", {
  tolerance = 1e-12
  expect_equal(as.numeric(fityb1$residual), unname(fit1$residuals),tolerance = tolerance)
})

test_that("yblm returns correct residual degree of freedom", {
  tolerance = 1e-12
  expect_equal(as.numeric(fityb1$R_df), unname(fit1$df.residual),tolerance = tolerance)
})

test_that("yblm returns correct degree of freedom between groups", {
  tolerance = 1e-12
  expect_equal(as.numeric(fityb1$M_df), unname(sum(anova(fit1)[1]) - fit1$df.residual),tolerance = tolerance)
})

test_that("yblm returns correct R_square", {
  tolerance = 1e-12
  expect_equal(as.numeric(fityb1$R_square),as.numeric(summary(fit1)[8]) ,tolerance = tolerance)
})

test_that("yblm returns correct Adj_R_square", {
  tolerance = 1e-12
  expect_equal(as.numeric(fityb1$Adj_R_square), as.numeric(summary(fit1)[9]),tolerance = tolerance)
})

test_that("yblm returns correct F_statistic", {
  tolerance = 1e-12
  expect_equal(as.numeric(fityb1$F_statistic), unname(summary(fit1)$fstatistic["value"]),tolerance = tolerance)
})

test_that("yblm returns correct F_statistic p value", {
  tolerance = 1e-5
  F_p_value <- pf(summary(fit1)$fstatistic[1], summary(fit1)$fstatistic[2], summary(fit1)$fstatistic[3], lower.tail = FALSE)
  expect_equal(as.numeric(fityb1$p_value_F), unname(F_p_value),tolerance = tolerance)
})

test_that("yblm returns correct coefficient matrix", {
  tolerance = 1e-12
  expect_equal(unname(fityb1$coef_matrix), unname(summary(fit1)$coefficients),tolerance = tolerance)
})
```

We can see that all the results are the same. Hence we justify the correctness of the function. Also we can compare the efficiency here. As the data structure are not the same, we cannot directly use benchmark to compare. However, we can use sys.time to compare the speed of the functions as a proof of efficiency.

```{r}
start = Sys.time()
fit1 = lm(Y~X,data = example_data1)
end = Sys.time()
fit1_speed = end - start
#fityb is the result generated by yblm()
start2 = Sys.time()
fityb1 = yblm(X,Y,TRUE)
end2 = Sys.time()
ybfit_speed = end2 - start2

ybfit_speed - fit1_speed
```

We can tell that the time `yblm` uses is smaller than that used by `lm()`, which proves the efficiency.

In the next example, we will use a public dataset called `iris` to do linear regression without intercept and justify the correctness and effiency of `yblm`.

```{r, collapse=TRUE}
example_data2 = iris
#You can run the code to check the dataset, we comment out the code since it is too long to be viewed in Rmd
#example_data2
```

We can see the sample data above. Then, we will set X = (Sepal.Width,Petal.Length,Petal.Width), Y = Sepal.Length from the example data. And then, we will run the linear regression on the example dataset using both `yblm()` and `lm()`. We will first try the regression with intercept.

```{r, collapse=TRUE}
X = as.matrix(example_data2[,2:4])
Y = as.matrix(example_data2[,1])
#fit1 is the result generated by lm()
fit2 = lm(Y~X + 0,data = example_data2)

#fityb is the result generated by yblm()
fityb2 = yblm(X,Y)
# A matrix containing value, Estimate standard error, t value and Pr(>|t|) of each coefficients
fityb2$coef_matrix
# The predicted values by the model
#You can run the code to check the dataset, we comment out the code since it is too long to be viewed in Rmd
# fityb2$predict
# The residuals, which is response minus predicted values.
#You can run the code to check the dataset, we comment out the code since it is too long to be viewed in Rmd
# fityb2$residual
# Sum of residuals
fityb2$SS_X
# The residual degree of freedom.
fityb2$R_df
# The degree of freedom between groups.
fityb2$M_df
# The standard error of the residual
fityb2$RSE
# The R square score.
fityb2$R_square
# The adjusted R square
fityb2$Adj_R_square
# The F statistic
fityb2$F_statistic
# The coefficient of the variables in the model
fityb2$coef
# p value of F-statistic
fityb2$p_value_F
```

We can see that `yblm` will return a list of objects that is informative in the linear regression, including predicted value, p-value and so on. Then we will compare the solution from the `yblm` and `lm` to justify the correctness and efficiency. As we are comparing floating number., we will set tolerance to compare two floating numbers.

```{r}
#----------------------------------------Test result when we have no intercept--------------------
test_that("yblm returns correct coefficient", {
  tolerance = 1e-12
  expect_equal(as.numeric(fityb2$coef), unname(fit2$coefficients),tolerance = tolerance)
})

test_that("yblm returns correct predicted value", {
  tolerance = 1e-12
  expect_equal(as.numeric(fityb2$predict), unname(fit2$fitted.values),tolerance = tolerance)
})

test_that("yblm returns correct residual value", {
  tolerance = 1e-12
  expect_equal(as.numeric(fityb2$residual), unname(fit2$residuals),tolerance = tolerance)
})

test_that("yblm returns correct residual degree of freedom", {
  tolerance = 1e-12
  expect_equal(as.numeric(fityb2$R_df), unname(fit2$df.residual),tolerance = tolerance)
})

test_that("yblm returns correct degree of freedom between groups", {
  tolerance = 1e-12
  expect_equal(as.numeric(fityb2$M_df), unname(sum(anova(fit2)[1]) - fit2$df.residual),tolerance = tolerance)
})

test_that("yblm returns correct R_square", {
  tolerance = 1e-12
  expect_equal(as.numeric(fityb2$R_square),as.numeric(summary(fit2)[8]) ,tolerance = tolerance)
})

test_that("yblm returns correct Adj_R_square", {
  tolerance = 1e-12
  expect_equal(as.numeric(fityb2$Adj_R_square), as.numeric(summary(fit2)[9]),tolerance = tolerance)
})

test_that("yblm returns correct F_statistic", {
  tolerance = 1e-12
  expect_equal(as.numeric(fityb2$F_statistic), unname(summary(fit2)$fstatistic["value"]),tolerance = tolerance)
})

test_that("yblm returns correct F_statistic p value", {
  tolerance = 1e-12
  F_p_value <- pf(summary(fit2)$fstatistic[1], summary(fit2)$fstatistic[2], summary(fit2)$fstatistic[3], lower.tail = FALSE)
  expect_equal(as.numeric(fityb2$p_value_F), unname(F_p_value),tolerance = tolerance)
})

test_that("yblm returns correct coefficient matrix", {
  tolerance = 1e-12
  expect_equal(unname(fityb2$coef_matrix), unname(summary(fit2)$coefficients),tolerance = tolerance)
})

```

We can see that all the results are the same. Hence we justify the correctness of the function. Also we can compare the efficiency here. As the data structure are not the same, we cannot directly use benchmark to compare. However, we can use sys.time to compare the speed of the functions as a proof of efficiency.

```{r}
start = Sys.time()
fit2 = lm(Y~X1 + X2 + X3+0,data = example_data1)
end = Sys.time()
fit2_speed = end - start
#fityb is the result generated by yblm()
start2 = Sys.time()
fityb1 = yblm(X,Y)
end2 = Sys.time()
ybfit2_speed = end2 - start2

ybfit2_speed - fit1_speed
```

We can tell that the time `yblm` uses is smaller than that used by `lm()`, which proves the efficiency.

# Introduction of the `ybanova` in the package

First, we need to know that this function is generally used in form of `ybanova(fit_model)`. `fit_model` is a model object got from the function `ybanova`. This function will automatically generate an ANOVA table for you. Notice that `ybanova` is written in cpp file, you will need to check src folder to look for the source code.

To know more about this package we can have two examples, we will go through the detail of them and justify the correctness and efficiency at the same time.

One example will be based on dataset `iris`. We have proved that `lm()` and `yblm()` will generate the same result, hence we still generate two fit model and compare their ANOVA table for correctness and effiency check.

```{r}
data_iris = iris
X = as.matrix(data_iris[,2:4])
Y = as.matrix(data_iris[,1])
#fit1 is the result generated by lm()
fit3 = lm(Y~X + 0,data = example_data2)
anova1 = anova(fit3)
#fityb is the result generated by yblm()
fityb3 = yblm(X,Y)
ybanova = ybanova(fityb3)
ybanova
```

Then we check the correctness and efficiency

```{r}
test_that("yblm returns correct table", {
  tolerance = 1e-5
  expect_equal(as.numeric(as.matrix(ybanova)[,-1]), as.numeric(as.matrix(anova1)),tolerance = tolerance)
})

start = Sys.time()
anova1 = anova(fit3)
end = Sys.time()
fit2_speed = end - start
#fityb is the result generated by yblm()
start2 = Sys.time()
ybanova = ybanova(fityb3)
end2 = Sys.time()
ybfit2_speed = end2 - start2

ybfit2_speed - fit1_speed
```

We can tell that all numbers are the same and ybanova is faster than normal anova. Hence we showed the efficiency and correctness.

Another example will be based on a random dataset . We have proved that `lm()` and `yblm()` will generate the same result, hence we still generate two fit model with intercept and compare their ANOVA table for correctness and efficiency check.

```{r}
X1 = rnorm(1000,0,1)
X2 = rnorm(1000,2,1)
X3 = rnorm(1000,4,2)

Y = rnorm(1000,2,3)

X = matrix(c(X1,X2,X3),ncol = 3)
Y = matrix(Y,ncol = 1)

#fit1 is the result generated by lm()
fit4 = lm(Y ~ X,data = data.frame(cbind(X,Y)))
anova2 = anova(fit4)
#fityb is the result generated by yblm()
fityb4 = yblm(X,Y,T)
ybanova2 = ybanova(fityb4)
ybanova2
```

Then we check the correctness and efficiency

```{r}
test_that("yblm returns correct table", {
  tolerance = 1e-5
  expect_equal(as.numeric(as.matrix(ybanova2)[,-1]), as.numeric(as.matrix(anova2)),tolerance = tolerance)
})

start = Sys.time()
anova2 = anova(fit4)
end = Sys.time()
fit2_speed = end - start
#fityb is the result generated by yblm()
start2 = Sys.time()
ybanov2 = ybanova(fityb4)
end2 = Sys.time()
ybfit2_speed = end2 - start2

ybfit2_speed - fit1_speed
```

We can tell that all numbers are the same and ybanova is faster than normal anova. Hence we showed the efficiency and correctness.

# Conclusion

We proved the correctness and efficiency of the function `yblm()` and `ybanova`. And this functions can help dealing with some linear regression problem in the real world if we want to get the information faster.
